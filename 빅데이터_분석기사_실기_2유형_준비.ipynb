{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP6wc6xEM9FTuExNqhQKk9K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pgs2285/BigData_Analysis_License/blob/master/%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0_%EB%B6%84%EC%84%9D%EA%B8%B0%EC%82%AC_%EC%8B%A4%EA%B8%B0_2%EC%9C%A0%ED%98%95_%EC%A4%80%EB%B9%84.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 유형 / 데이터 분석\n",
        "\n",
        "------  \n",
        "분석문제는 다음과 같은 순서를 따르자.\n",
        "1. 필요 패키지 임포트  \n",
        "2. 데이터 불러오기  \n",
        "3. 데이터 살펴보기\n",
        "4. 데이터 전처리\n",
        "5. 분석 데이터셋 준비\n",
        "6. 데이터 분석 수행\n",
        "7. 성능평가 및 시각화\n",
        "------  \n",
        "아래는 iris 데이터를 random forest 로 분류한 예시이다."
      ],
      "metadata": {
        "id": "qBeigfMFFeW7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_sh7nn95Fb8h",
        "outputId": "1886d38a-6302-496d-e627-a7a6cfed7889"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 150 entries, 0 to 149\n",
            "Data columns (total 5 columns):\n",
            " #   Column        Non-Null Count  Dtype  \n",
            "---  ------        --------------  -----  \n",
            " 0   sepal_length  150 non-null    float64\n",
            " 1   sepal_width   150 non-null    float64\n",
            " 2   petal_length  150 non-null    float64\n",
            " 3   petal_width   150 non-null    float64\n",
            " 4   species       150 non-null    object \n",
            "dtypes: float64(4), object(1)\n",
            "memory usage: 6.0+ KB\n",
            "None\n",
            "0.9333333333333333\n",
            "[[ 9  0  0]\n",
            " [ 0 10  0]\n",
            " [ 0  2  9]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00         9\n",
            "           1       0.83      1.00      0.91        10\n",
            "           2       1.00      0.82      0.90        11\n",
            "\n",
            "    accuracy                           0.93        30\n",
            "   macro avg       0.94      0.94      0.94        30\n",
            "weighted avg       0.94      0.93      0.93        30\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 1. 필요 패키지 임포트\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 2. 데이터 불러오기\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv\")\n",
        "\n",
        "# 3. 데이터 살펴보기\n",
        "print(df.info())\n",
        "\n",
        "# 4. 데이터 전처리\n",
        "df['species'].replace({\"setosa\" : 0, \"versicolor\" : 1, \"virginica\" : 2}, inplace = True)\n",
        "\n",
        "# 5. 분석 데이터셋 준비\n",
        "X = df[[\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]]\n",
        "y = df[\"species\"]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2, random_state=11)\n",
        "\n",
        "# 6.데이터분석 수행\n",
        "dt = DecisionTreeClassifier(random_state = 11)\n",
        "\n",
        "dt.fit(X_train, y_train)\n",
        "\n",
        "pred = dt.predict(X_test)\n",
        "\n",
        "#7. 성능평가 및 시각화\n",
        "#Accuracy (TP+TN / TP+FP+TN+FN)\n",
        "from sklearn.metrics import accuracy_score\n",
        "acc = accuracy_score(y_test,pred)\n",
        "print(acc)\n",
        "\n",
        "#오차행렬\n",
        "from sklearn.metrics import confusion_matrix\n",
        "print(confusion_matrix(y_test, pred))\n",
        "\n",
        "#오차행렬에 기반한 모델 평가지표\n",
        "from sklearn.metrics import classification_report\n",
        "rpt = classification_report(y_test,pred)\n",
        "print(rpt)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 지도학습 - 분류  \n",
        "지도학습은 정답이 있는 데이터가 주어진 상태에서 학습하는 방법이다. 크게 분류와 회귀(예측)로 구분할 수 있다.  \n",
        "분류란 기존에 존재하는 데이터들간의 분류 카테고리를 학습, 파악하고 새로운 데이터에 대한 분류 카테고리를 판별하는 과정이다.  \n",
        "분류는 학습 데이터를 기반으로 분류모델을 생성하고, 생성된 모델이 새로운 데이터 값이 주어졌을 때 결정값을 판별한다.  \n",
        "* 이진분류 :  주어진 데이터에 대해 두 가지 중 하나로 분류하는것.  \n",
        "* 다중분류 :  주어진 데이터에 대해 여러 가지 중 하나로 분류하는 것.  \n",
        "  \n",
        "다음과 같은 알고리즘들이 있다 -> 의사결정나무(Decision Tree), KNN(K-Nearest Neighbor), 로지스틱 회귀(Logistic Regression), 랜덤 포레스트(Random Forest),\n",
        "나이브 베이즈(Naive Bayes), 신경망(Neural Network), 서포트벡터머신(SVM, Support Vector Machine)  \n",
        "\n",
        "\n",
        "------\n",
        "#### 의사결정나무\n",
        "\n",
        "##### 알고리즘 개요  \n",
        "* 의사결정을 위한 규칙을 나무 모양으로 조합하여 목표변수에 대한 분류를 수행하는 기법이다.  \n",
        "* 목표변수가 이산형인 경우로 의사결정나무를 학습하는 것은 최종적으로 분류나무를 구축하는 과정이다.\n",
        "* 의사결정나무 기법을 사용한 분석은 시장조사, 광고조사, 품질관리 등 다양한 분야에서 활용되고 있으며, 타겟 고객 분류, 고객 신용 분류, 행동 예측 등에 사용된다.  \n",
        "\n",
        "##### 알고리즘 특징  \n",
        "* 수학적 지식이 없어도 결과를 해석하고 이해하기 쉽다.  \n",
        "* 수치 데이터 및 범주 데이터에 모두 사용 가능하다.\n",
        "* 과대 적합의 위험이 높기 때문에, 학습모델이 과대적합 되지 않도록 적절히 조절되어야 한다.  \n",
        "\n",
        "> 랜덤포레스트\n",
        "> 부트스트래핑 기반 샘플링을 활용한 의사결정나무 생성 이후 배깅 기반 나무들을 모아 앙상블 학습하여 숲을 형성하게 되면 이를 랜덤포레스트라고 한다.\n",
        "\n"
      ],
      "metadata": {
        "id": "nsJltivjRGex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 의사결정나무를 이용한 타이타닉 데이터셋에서 탑습자들의 여러 속성데이터를 기반으로 생존여부를 예측한다.\n",
        "'''\n",
        " 접근방법\n",
        "타이타닉 데이터셋은 여러 칼럼으로 구성되어 있으며, 이를 기반으로 Survived 칼럼의 값을 예측한다.\n",
        "불필요한 속성은 제거하고 전처리과정을 거친 후, 사이킷런의 의사결정나무 알고리즘을 이용하여 학습 모델을 구축한 후 예측을 수행한다.\n",
        "'''\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\")\n",
        "df.describe()\n",
        "\n",
        "d_mean = np.mean(df[\"Age\"])\n",
        "df[\"Age\"].fillna(d_mean,inplace=True)\n",
        "df[\"Embarked\"].fillna(df[\"Embarked\"].mode()[0], inplace=True)\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "df[\"Sex\"] = LabelEncoder().fit_transform(df[\"Sex\"])\n",
        "df[\"Embarked\"] = LabelEncoder().fit_transform(df[\"Embarked\"])\n",
        "df[\"FamilySize\"] = df[\"SibSp\"] + df[\"Parch\"]\n",
        "\n",
        "X = df[[\"Pclass\", \"Sex\", \"Age\", \"Fare\",\"Embarked\",\"FamilySize\"]]\n",
        "y = df[\"Survived\"]\n",
        "\n",
        "train_x,test_x,train_y,test_y = train_test_split(X,y,test_size=0.2,random_state=11)\n",
        "\n",
        "model = DecisionTreeClassifier(random_state=11)\n",
        "model.fit(train_x,train_y)\n",
        "pred = model.predict(test_x)\n",
        "from sklearn.metrics import accuracy_score\n",
        "acc = accuracy_score(test_y,pred)\n",
        "\n",
        "cnt = 0\n",
        "for i,j in enumerate(pred):\n",
        "  if j == test_y.iloc[i]:\n",
        "    cnt += 1\n",
        "print(acc,(cnt/test_x.shape[0]))\n"
      ],
      "metadata": {
        "id": "LnWw6IH-RF4l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5d7015d-edc8-4d02-ba90-c1a0951617bf"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7877094972067039 0.7877094972067039\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### KNN 알고리즘"
      ],
      "metadata": {
        "id": "PesqqK_GUh-u"
      }
    }
  ]
}