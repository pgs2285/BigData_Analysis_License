{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNZZMd5cWPmKSy20hxlB1gl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pgs2285/BigData_Analysis_License/blob/master/%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0_%EB%B6%84%EC%84%9D%EA%B8%B0%EC%82%AC_%EC%8B%A4%EA%B8%B0_2%EC%9C%A0%ED%98%95_%EC%A4%80%EB%B9%84.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 유형 / 데이터 분석\n",
        "\n",
        "------  \n",
        "분석문제는 다음과 같은 순서를 따르자.\n",
        "1. 필요 패키지 임포트  \n",
        "2. 데이터 불러오기  \n",
        "3. 데이터 살펴보기\n",
        "4. 데이터 전처리\n",
        "5. 분석 데이터셋 준비\n",
        "6. 데이터 분석 수행\n",
        "7. 성능평가 및 시각화\n",
        "------  \n",
        "아래는 iris 데이터를 random forest 로 분류한 예시이다."
      ],
      "metadata": {
        "id": "qBeigfMFFeW7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_sh7nn95Fb8h",
        "outputId": "1886d38a-6302-496d-e627-a7a6cfed7889"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 150 entries, 0 to 149\n",
            "Data columns (total 5 columns):\n",
            " #   Column        Non-Null Count  Dtype  \n",
            "---  ------        --------------  -----  \n",
            " 0   sepal_length  150 non-null    float64\n",
            " 1   sepal_width   150 non-null    float64\n",
            " 2   petal_length  150 non-null    float64\n",
            " 3   petal_width   150 non-null    float64\n",
            " 4   species       150 non-null    object \n",
            "dtypes: float64(4), object(1)\n",
            "memory usage: 6.0+ KB\n",
            "None\n",
            "0.9333333333333333\n",
            "[[ 9  0  0]\n",
            " [ 0 10  0]\n",
            " [ 0  2  9]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00         9\n",
            "           1       0.83      1.00      0.91        10\n",
            "           2       1.00      0.82      0.90        11\n",
            "\n",
            "    accuracy                           0.93        30\n",
            "   macro avg       0.94      0.94      0.94        30\n",
            "weighted avg       0.94      0.93      0.93        30\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 1. 필요 패키지 임포트\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 2. 데이터 불러오기\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv\")\n",
        "\n",
        "# 3. 데이터 살펴보기\n",
        "print(df.info())\n",
        "\n",
        "# 4. 데이터 전처리\n",
        "df['species'].replace({\"setosa\" : 0, \"versicolor\" : 1, \"virginica\" : 2}, inplace = True)\n",
        "\n",
        "# 5. 분석 데이터셋 준비\n",
        "X = df[[\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]]\n",
        "y = df[\"species\"]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2, random_state=11)\n",
        "\n",
        "# 6.데이터분석 수행\n",
        "dt = DecisionTreeClassifier(random_state = 11)\n",
        "\n",
        "dt.fit(X_train, y_train)\n",
        "\n",
        "pred = dt.predict(X_test)\n",
        "\n",
        "#7. 성능평가 및 시각화\n",
        "#Accuracy (TP+TN / TP+FP+TN+FN)\n",
        "from sklearn.metrics import accuracy_score\n",
        "acc = accuracy_score(y_test,pred)\n",
        "print(acc)\n",
        "\n",
        "#오차행렬\n",
        "from sklearn.metrics import confusion_matrix\n",
        "print(confusion_matrix(y_test, pred))\n",
        "\n",
        "#오차행렬에 기반한 모델 평가지표\n",
        "from sklearn.metrics import classification_report\n",
        "rpt = classification_report(y_test,pred)\n",
        "print(rpt)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 지도학습 - 분류  \n",
        "지도학습은 정답이 있는 데이터가 주어진 상태에서 학습하는 방법이다. 크게 분류와 회귀(예측)로 구분할 수 있다.  \n",
        "분류란 기존에 존재하는 데이터들간의 분류 카테고리를 학습, 파악하고 새로운 데이터에 대한 분류 카테고리를 판별하는 과정이다.  \n",
        "분류는 학습 데이터를 기반으로 분류모델을 생성하고, 생성된 모델이 새로운 데이터 값이 주어졌을 때 결정값을 판별한다.  \n",
        "* 이진분류 :  주어진 데이터에 대해 두 가지 중 하나로 분류하는것.  \n",
        "* 다중분류 :  주어진 데이터에 대해 여러 가지 중 하나로 분류하는 것.  \n",
        "  \n",
        "다음과 같은 알고리즘들이 있다 -> 의사결정나무(Decision Tree), KNN(K-Nearest Neighbor), 로지스틱 회귀(Logistic Regression), 랜덤 포레스트(Random Forest),\n",
        "나이브 베이즈(Naive Bayes), 신경망(Neural Network), 서포트벡터머신(SVM, Support Vector Machine)  \n",
        "\n",
        "\n",
        "------\n",
        "#### 의사결정나무\n",
        "\n",
        "##### 알고리즘 개요  \n",
        "* 의사결정을 위한 규칙을 나무 모양으로 조합하여 목표변수에 대한 분류를 수행하는 기법이다.  \n",
        "* 목표변수가 이산형인 경우로 의사결정나무를 학습하는 것은 최종적으로 분류나무를 구축하는 과정이다.\n",
        "* 의사결정나무 기법을 사용한 분석은 시장조사, 광고조사, 품질관리 등 다양한 분야에서 활용되고 있으며, 타겟 고객 분류, 고객 신용 분류, 행동 예측 등에 사용된다.  \n",
        "\n",
        "##### 알고리즘 특징  \n",
        "* 수학적 지식이 없어도 결과를 해석하고 이해하기 쉽다.  \n",
        "* 수치 데이터 및 범주 데이터에 모두 사용 가능하다.\n",
        "* 과대 적합의 위험이 높기 때문에, 학습모델이 과대적합 되지 않도록 적절히 조절되어야 한다.  \n",
        "\n",
        "> 랜덤포레스트\n",
        "> 부트스트래핑 기반 샘플링을 활용한 의사결정나무 생성 이후 배깅 기반 나무들을 모아 앙상블 학습하여 숲을 형성하게 되면 이를 랜덤포레스트라고 한다.\n",
        "\n"
      ],
      "metadata": {
        "id": "nsJltivjRGex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 의사결정나무를 이용한 타이타닉 데이터셋에서 탑습자들의 여러 속성데이터를 기반으로 생존여부를 예측한다.\n",
        "'''\n",
        " 접근방법\n",
        "타이타닉 데이터셋은 여러 칼럼으로 구성되어 있으며, 이를 기반으로 Survived 칼럼의 값을 예측한다.\n",
        "불필요한 속성은 제거하고 전처리과정을 거친 후, 사이킷런의 의사결정나무 알고리즘을 이용하여 학습 모델을 구축한 후 예측을 수행한다.\n",
        "'''\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\")\n",
        "df.describe()\n",
        "\n",
        "d_mean = np.mean(df[\"Age\"])\n",
        "df[\"Age\"].fillna(d_mean,inplace=True)\n",
        "df[\"Embarked\"].fillna(df[\"Embarked\"].mode()[0], inplace=True)\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "df[\"Sex\"] = LabelEncoder().fit_transform(df[\"Sex\"])\n",
        "df[\"Embarked\"] = LabelEncoder().fit_transform(df[\"Embarked\"])\n",
        "df[\"FamilySize\"] = df[\"SibSp\"] + df[\"Parch\"]\n",
        "\n",
        "X = df[[\"Pclass\", \"Sex\", \"Age\", \"Fare\",\"Embarked\",\"FamilySize\"]]\n",
        "y = df[\"Survived\"]\n",
        "\n",
        "train_x,test_x,train_y,test_y = train_test_split(X,y,test_size=0.2,random_state=11)\n",
        "\n",
        "model = DecisionTreeClassifier(random_state=11)\n",
        "model.fit(train_x,train_y)\n",
        "pred = model.predict(test_x)\n",
        "from sklearn.metrics import accuracy_score\n",
        "acc = accuracy_score(test_y,pred)\n",
        "\n",
        "cnt = 0\n",
        "for i,j in enumerate(pred):\n",
        "  if j == test_y.iloc[i]:\n",
        "    cnt += 1\n",
        "print(acc,(cnt/test_x.shape[0]))\n"
      ],
      "metadata": {
        "id": "LnWw6IH-RF4l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5d7015d-edc8-4d02-ba90-c1a0951617bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7877094972067039 0.7877094972067039\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### KNN 알고리즘  \n",
        "##### 알고리즘 개요  \n",
        "* KNN 알고리즘은 지도학습의 한 종류로, 정답이 있는 데이터를 사용하여 분류 작업을 한다.\n",
        "* 서로 가가운 점은 유사하다는 가정하에, 데이터로부터 거리가 가까운 K개의 다른 데이터의 정답을 참조하여 분류한다.  \n",
        "> KNN알고리즘은 변수별 단위가 무엇이냐에 따라 거리가 달라지고 분류 결과가 달라질 수 있다.\n",
        "따라서 KNN 알고리즘을 적용할때는 사전에 데이터를 표준화 해야한다.\n",
        "\n",
        "##### 알고리즘 특징  \n",
        "* 동작원리가 단순하여 이해하기 쉬우며, 알고리즘이 간단하여 구현하기 쉽다.\n",
        "* 거리 기반의 연산으로, 숫자로 구분된 속성에 우수한 성능을 보인다.  \n",
        "* 하나의 데이터를 예측할 때마다 전체 데이터와의 거리를 계산하기 때문에 차원(벡터)의 크기가 크면 계산량이 많아진다.  "
      ],
      "metadata": {
        "id": "PesqqK_GUh-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# iris 데이터셋을 이용하여 꽃잎의 길이와 너비, 꽃받침의 길이와 너비를 가지고 붓꽃의 품종을 분류하는 문제를 KNN알고리즘을 사용해서 해결한다.\n",
        "# KNN 알고리즘은 K값에 따라서 분류의 정확도가 달라지므로, 적절한 K값을 찾는것이 매우 중요하다.\n",
        "# ex) k = 3이면 새로운 데이터로부터 가장 가까운 이웃 3개를 찾고, 그 중에서 가장 개수가 많은 값으로 분류한다.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv\")\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "df[\"sepal_length\"] = scaler.fit_transform(df[[\"sepal_length\"]])\n",
        "df[\"sepal_width\"] = scaler.fit_transform(df[[\"sepal_width\"]])\n",
        "df[\"petal_length\"] = scaler.fit_transform(df[[\"petal_length\"]])\n",
        "df[\"petal_width\"] = scaler.fit_transform(df[[\"petal_width\"]])\n",
        "\n",
        "X = df[[\"sepal_length\",\"sepal_width\",\"petal_length\",\"petal_width\"]]\n",
        "y = df[\"species\"]\n",
        "\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=11)\n",
        "print(X_train.shape, y_train.shape)\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors = 3)\n",
        "knn.fit(X_train,y_train)\n",
        "pred = knn.predict(X_test)\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "acc = accuracy_score(y_test, pred)\n",
        "print(acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pr-xx40Teglz",
        "outputId": "2e7f2dbd-37cb-4051-dc51-9d13069dc103"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(120, 4) (120,)\n",
            "0.9333333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### SVM(Support Vector Machine) 알고리즘\n",
        "##### 알고리즘 개요  \n",
        "* 서포트벡터머신은 기계학습의 분야 중 하나로 패턴 인식, 자료 분석을 위한 지도학습 모델이며, 주로 분류와 회귀 분석을 위해 사용한다.\n",
        "* 두 카테고리 중 어느 하나에 속한 데이터가 어느 카테고리에 속할지 판단하는 비확률적 이진 선형 분류 모델을 만든다.\n",
        "* 만들어진 분류 모델은 데이터가 사상된 공간에서 경계로 표현되는데 SVM알고리즘은 그 중 가장 큰 폭을 가진 경계를 찾는 알고리즘 이다.  \n",
        "##### 알고리즘 특징\n",
        "* 커널 트릭을 사용함으로써 다양한 데이터의 특성에 맞는 분류를 수행할 수 있다.\n",
        "* 비교적 적은 학습데이터로도 정확도가 높은 분류를 기대할 수 있다. (전처리 과정을 통해 데이터의 특성을 잘 표현해야한다)  \n",
        "* 변수가 많은경우 결정 경계 및 데이터의 시각화가 어려워 분류 결과의 이해가 어렵다."
      ],
      "metadata": {
        "id": "tw21vvFLBt_c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\")\n",
        "d_mean = np.mean(df[\"Age\"])\n",
        "df[\"Age\"].fillna(d_mean, inplace=True)\n",
        "df[\"Embarked\"].fillna(df[\"Embarked\"].mode()[0], inplace =True)\n",
        "df.drop(\"Cabin\", axis = 1, inplace =True)\n",
        "df[\"FamilySize\"] = df[\"SibSp\"] + df[\"Parch\"]\n",
        "\n",
        "onehot_Sex = pd.get_dummies(df[\"Sex\"])\n",
        "onehot_Embarked = pd.get_dummies(df[\"Embarked\"])\n",
        "\n",
        "df = pd.concat([df,onehot_Sex,onehot_Embarked], axis = 1)\n",
        "df.drop(\"Sex\", axis = 1, inplace = True)\n",
        "df.drop(\"Embarked\", axis = 1, inplace = True)\n",
        "\n",
        "X = df[[\"Pclass\", \"Age\", \"Fare\", \"FamilySize\",\"female\",\"male\",\"C\",\"Q\",\"S\"]]\n",
        "y = df[\"Survived\"]\n",
        "\n",
        "svc = SVC(kernel='rbf')\n",
        "x_train, x_test, y_train, y_test = train_test_split(X,y,test_size = 0.3, random_state =10\n",
        "                                                    )\n",
        "\n",
        "print(x_train.shape, y_train.shape)\n",
        "\n",
        "svc.fit(x_train, y_train)\n",
        "pred = svc.predict(x_test)\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "acc = accuracy_score(y_test, pred)\n",
        "print(\"정확도 \" , acc)\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "mat = confusion_matrix(y_test,pred)\n",
        "print(mat)\n",
        "\n",
        "rep = classification_report(y_test,pred)\n",
        "print(rep)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kk7WpXa3BtB8",
        "outputId": "252f8d64-f8eb-4f70-99ca-bba9f633cbdc"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(623, 9) (623,)\n",
            "정확도  0.7238805970149254\n",
            "[[167   7]\n",
            " [ 67  27]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.96      0.82       174\n",
            "           1       0.79      0.29      0.42        94\n",
            "\n",
            "    accuracy                           0.72       268\n",
            "   macro avg       0.75      0.62      0.62       268\n",
            "weighted avg       0.74      0.72      0.68       268\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7Ih_1_erIeRB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}