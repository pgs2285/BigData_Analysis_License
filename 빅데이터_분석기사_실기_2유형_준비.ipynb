{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN+QNwOFzPFVltHtqrAJLtS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pgs2285/BigData_Analysis_License/blob/master/%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0_%EB%B6%84%EC%84%9D%EA%B8%B0%EC%82%AC_%EC%8B%A4%EA%B8%B0_2%EC%9C%A0%ED%98%95_%EC%A4%80%EB%B9%84.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 유형 / 데이터 분석\n",
        "\n",
        "------  \n",
        "분석문제는 다음과 같은 순서를 따르자.\n",
        "1. 필요 패키지 임포트  \n",
        "2. 데이터 불러오기  \n",
        "3. 데이터 살펴보기\n",
        "4. 데이터 전처리\n",
        "5. 분석 데이터셋 준비\n",
        "6. 데이터 분석 수행\n",
        "7. 성능평가 및 시각화\n",
        "------  \n",
        "아래는 iris 데이터를 random forest 로 분류한 예시이다."
      ],
      "metadata": {
        "id": "qBeigfMFFeW7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_sh7nn95Fb8h",
        "outputId": "1886d38a-6302-496d-e627-a7a6cfed7889"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 150 entries, 0 to 149\n",
            "Data columns (total 5 columns):\n",
            " #   Column        Non-Null Count  Dtype  \n",
            "---  ------        --------------  -----  \n",
            " 0   sepal_length  150 non-null    float64\n",
            " 1   sepal_width   150 non-null    float64\n",
            " 2   petal_length  150 non-null    float64\n",
            " 3   petal_width   150 non-null    float64\n",
            " 4   species       150 non-null    object \n",
            "dtypes: float64(4), object(1)\n",
            "memory usage: 6.0+ KB\n",
            "None\n",
            "0.9333333333333333\n",
            "[[ 9  0  0]\n",
            " [ 0 10  0]\n",
            " [ 0  2  9]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00         9\n",
            "           1       0.83      1.00      0.91        10\n",
            "           2       1.00      0.82      0.90        11\n",
            "\n",
            "    accuracy                           0.93        30\n",
            "   macro avg       0.94      0.94      0.94        30\n",
            "weighted avg       0.94      0.93      0.93        30\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 1. 필요 패키지 임포트\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 2. 데이터 불러오기\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv\")\n",
        "\n",
        "# 3. 데이터 살펴보기\n",
        "print(df.info())\n",
        "\n",
        "# 4. 데이터 전처리\n",
        "df['species'].replace({\"setosa\" : 0, \"versicolor\" : 1, \"virginica\" : 2}, inplace = True)\n",
        "\n",
        "# 5. 분석 데이터셋 준비\n",
        "X = df[[\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]]\n",
        "y = df[\"species\"]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2, random_state=11)\n",
        "\n",
        "# 6.데이터분석 수행\n",
        "dt = DecisionTreeClassifier(random_state = 11)\n",
        "\n",
        "dt.fit(X_train, y_train)\n",
        "\n",
        "pred = dt.predict(X_test)\n",
        "\n",
        "#7. 성능평가 및 시각화\n",
        "#Accuracy (TP+TN / TP+FP+TN+FN)\n",
        "from sklearn.metrics import accuracy_score\n",
        "acc = accuracy_score(y_test,pred)\n",
        "print(acc)\n",
        "\n",
        "#오차행렬\n",
        "from sklearn.metrics import confusion_matrix\n",
        "print(confusion_matrix(y_test, pred))\n",
        "\n",
        "#오차행렬에 기반한 모델 평가지표\n",
        "from sklearn.metrics import classification_report\n",
        "rpt = classification_report(y_test,pred)\n",
        "print(rpt)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 지도학습 - 분류  \n",
        "지도학습은 정답이 있는 데이터가 주어진 상태에서 학습하는 방법이다. 크게 분류와 회귀(예측)로 구분할 수 있다.  \n",
        "분류란 기존에 존재하는 데이터들간의 분류 카테고리를 학습, 파악하고 새로운 데이터에 대한 분류 카테고리를 판별하는 과정이다.  \n",
        "분류는 학습 데이터를 기반으로 분류모델을 생성하고, 생성된 모델이 새로운 데이터 값이 주어졌을 때 결정값을 판별한다.  \n",
        "* 이진분류 :  주어진 데이터에 대해 두 가지 중 하나로 분류하는것.  \n",
        "* 다중분류 :  주어진 데이터에 대해 여러 가지 중 하나로 분류하는 것.  \n",
        "  \n",
        "다음과 같은 알고리즘들이 있다 -> 의사결정나무(Decision Tree), KNN(K-Nearest Neighbor), 로지스틱 회귀(Logistic Regression), 랜덤 포레스트(Random Forest),\n",
        "나이브 베이즈(Naive Bayes), 신경망(Neural Network), 서포트벡터머신(SVM, Support Vector Machine)  \n",
        "\n",
        "\n",
        "------\n",
        "#### 의사결정나무\n",
        "\n",
        "##### 알고리즘 개요  \n",
        "* 의사결정을 위한 규칙을 나무 모양으로 조합하여 목표변수에 대한 분류를 수행하는 기법이다.  \n",
        "* 목표변수가 이산형인 경우로 의사결정나무를 학습하는 것은 최종적으로 분류나무를 구축하는 과정이다.\n",
        "* 의사결정나무 기법을 사용한 분석은 시장조사, 광고조사, 품질관리 등 다양한 분야에서 활용되고 있으며, 타겟 고객 분류, 고객 신용 분류, 행동 예측 등에 사용된다.  \n",
        "\n",
        "##### 알고리즘 특징  \n",
        "* 수학적 지식이 없어도 결과를 해석하고 이해하기 쉽다.  \n",
        "* 수치 데이터 및 범주 데이터에 모두 사용 가능하다.\n",
        "* 과대 적합의 위험이 높기 때문에, 학습모델이 과대적합 되지 않도록 적절히 조절되어야 한다.  \n",
        "\n",
        "> 랜덤포레스트\n",
        "> 부트스트래핑 기반 샘플링을 활용한 의사결정나무 생성 이후 배깅 기반 나무들을 모아 앙상블 학습하여 숲을 형성하게 되면 이를 랜덤포레스트라고 한다.\n",
        "\n"
      ],
      "metadata": {
        "id": "nsJltivjRGex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 의사결정나무를 이용한 타이타닉 데이터셋에서 탑습자들의 여러 속성데이터를 기반으로 생존여부를 예측한다.\n",
        "'''\n",
        " 접근방법\n",
        "타이타닉 데이터셋은 여러 칼럼으로 구성되어 있으며, 이를 기반으로 Survived 칼럼의 값을 예측한다.\n",
        "불필요한 속성은 제거하고 전처리과정을 거친 후, 사이킷런의 의사결정나무 알고리즘을 이용하여 학습 모델을 구축한 후 예측을 수행한다.\n",
        "'''\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\")\n",
        "df.describe()\n",
        "\n",
        "d_mean = np.mean(df[\"Age\"])\n",
        "df[\"Age\"].fillna(d_mean,inplace=True)\n",
        "df[\"Embarked\"].fillna(df[\"Embarked\"].mode()[0], inplace=True)\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "df[\"Sex\"] = LabelEncoder().fit_transform(df[\"Sex\"])\n",
        "df[\"Embarked\"] = LabelEncoder().fit_transform(df[\"Embarked\"])\n",
        "df[\"FamilySize\"] = df[\"SibSp\"] + df[\"Parch\"]\n",
        "\n",
        "X = df[[\"Pclass\", \"Sex\", \"Age\", \"Fare\",\"Embarked\",\"FamilySize\"]]\n",
        "y = df[\"Survived\"]\n",
        "\n",
        "train_x,test_x,train_y,test_y = train_test_split(X,y,test_size=0.2,random_state=11)\n",
        "\n",
        "model = DecisionTreeClassifier(random_state=11)\n",
        "model.fit(train_x,train_y)\n",
        "pred = model.predict(test_x)\n",
        "from sklearn.metrics import accuracy_score\n",
        "acc = accuracy_score(test_y,pred)\n",
        "\n",
        "cnt = 0\n",
        "for i,j in enumerate(pred):\n",
        "  if j == test_y.iloc[i]:\n",
        "    cnt += 1\n",
        "print(acc,(cnt/test_x.shape[0]))\n"
      ],
      "metadata": {
        "id": "LnWw6IH-RF4l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5d7015d-edc8-4d02-ba90-c1a0951617bf"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7877094972067039 0.7877094972067039\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### KNN 알고리즘  \n",
        "##### 알고리즘 개요  \n",
        "* KNN 알고리즘은 지도학습의 한 종류로, 정답이 있는 데이터를 사용하여 분류 작업을 한다.\n",
        "* 서로 가가운 점은 유사하다는 가정하에, 데이터로부터 거리가 가까운 K개의 다른 데이터의 정답을 참조하여 분류한다.  \n",
        "> KNN알고리즘은 변수별 단위가 무엇이냐에 따라 거리가 달라지고 분류 결과가 달라질 수 있다.\n",
        "따라서 KNN 알고리즘을 적용할때는 사전에 데이터를 표준화 해야한다.\n",
        "\n",
        "##### 알고리즘 특징  \n",
        "* 동작원리가 단순하여 이해하기 쉬우며, 알고리즘이 간단하여 구현하기 쉽다.\n",
        "* 거리 기반의 연산으로, 숫자로 구분된 속성에 우수한 성능을 보인다.  \n",
        "* 하나의 데이터를 예측할 때마다 전체 데이터와의 거리를 계산하기 때문에 차원(벡터)의 크기가 크면 계산량이 많아진다.  "
      ],
      "metadata": {
        "id": "PesqqK_GUh-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# iris 데이터셋을 이용하여 꽃잎의 길이와 너비, 꽃받침의 길이와 너비를 가지고 붓꽃의 품종을 분류하는 문제를 KNN알고리즘을 사용해서 해결한다.\n",
        "# KNN 알고리즘은 K값에 따라서 분류의 정확도가 달라지므로, 적절한 K값을 찾는것이 매우 중요하다.\n",
        "# ex) k = 3이면 새로운 데이터로부터 가장 가까운 이웃 3개를 찾고, 그 중에서 가장 개수가 많은 값으로 분류한다.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv\")\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "df[\"sepal_length\"] = scaler.fit_transform(df[[\"sepal_length\"]])\n",
        "df[\"sepal_width\"] = scaler.fit_transform(df[[\"sepal_width\"]])\n",
        "df[\"petal_length\"] = scaler.fit_transform(df[[\"petal_length\"]])\n",
        "df[\"petal_width\"] = scaler.fit_transform(df[[\"petal_width\"]])\n",
        "\n",
        "X = df[[\"sepal_length\",\"sepal_width\",\"petal_length\",\"petal_width\"]]\n",
        "y = df[\"species\"]\n",
        "\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=11)\n",
        "print(X_train.shape, y_train.shape)\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbor = 3)\n",
        "knn.fit(X_train,y_train)\n",
        "pred = knn.predict(x_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pr-xx40Teglz",
        "outputId": "0edf1d3b-8c29-4d6e-ed4f-7a483d7f2e8c"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        sepal_length  sepal_width  petal_length  petal_width species\n",
            "count     150.000000   150.000000    150.000000   150.000000     150\n",
            "unique           NaN          NaN           NaN          NaN       3\n",
            "top              NaN          NaN           NaN          NaN  setosa\n",
            "freq             NaN          NaN           NaN          NaN      50\n",
            "mean        0.428704     0.440556      0.467458     0.458056     NaN\n",
            "std         0.230018     0.181611      0.299203     0.317599     NaN\n",
            "min         0.000000     0.000000      0.000000     0.000000     NaN\n",
            "25%         0.222222     0.333333      0.101695     0.083333     NaN\n",
            "50%         0.416667     0.416667      0.567797     0.500000     NaN\n",
            "75%         0.583333     0.541667      0.694915     0.708333     NaN\n",
            "max         1.000000     1.000000      1.000000     1.000000     NaN\n",
            "(120, 4) (120,)\n"
          ]
        }
      ]
    }
  ]
}